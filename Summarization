==============================================================
Text Generation from Knowledge Graphs with Graph Transformerst
==============================================================

1. architectures
1) encoders: CNN, LSTM, Transformer
2) decoders: auto-regressive, non auto-regressive

2. 
1) external transferable knowledge
Glove, BERT, different learning schemas (supervised learning, reinforcement learning)
CNN/DailyMail
position information
2) unsupervised transferable knowledge
(How to use transformer in extractive summerazition?)

3.
1) sentence encoder --> 
doesn't matter

document encoder --> 
LSTM-based
transformer

decoder 
Sequence Labeling(SeqLab): auto-regressive
Pointer Network

======================================================
A Survey on Neural Network-Based Summarization Methods
======================================================
Evaluation
1) intrinsic: system-generated summaries v.s. human-created "gold" summaries: ROUGE, Pyramid
2) extrinsic: based on the performance of the down-stream task

* Neural-Based Summarization Techniques
word embedding: Word2Vec, CW vectors, GloVe
--> encoder: sentence/document: CNNsï¼ŒRNNs; Recursive Auto-encoder(RAE) 
--> model: selection/generation

1) sentence selection
* diversity of the sentence
* coverage of the input text
 
=========================================================================
Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting
=========================================================================
released code, best pretrained models, output summaries: https://github.com/ChenRocks/fast_abs_rl
a hybrid extractive-abstractive architecture
#   extractor agent  #: 1) select salient sentences
# abstractor network #: 2) rewrite them abstractively

parallel decoding: sentence level & word level
embedded word vectors --(convolutional encoder)--> sentence encoder
--(RNN encoder)--> context-aware representation --(RNN decoder)--> select sentence

# Extractor Agent
## Hierarchical Sentence Representation:
## Sentence Selection: LSTM-RNN to train a Pointer Network
# Abstractor Network



