=========================================
Recent Advances in Document Summarization
=========================================
Challenge
---------
1. collecting data
. collecting large-scale data
. query-focused summarization dataset --> query relevance model ====> unsupervised?
2. improving evaluation
. ROUGE: information coverage
. coherence
3. understanding ====> kg
. semantic-level process > simlilarities or overlaps
. discourse parsing
4. encoding
. hierarchical encoding and attention, external memory units for storing distant but more significant information
5. at scale
6. with user interactions
. query-chain summarization task

===========================================================
Topic Concentration in Query Focused Summarization Datasets
===========================================================

=============================================================================================
Earlier Isn’t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization
=============================================================================================
summarization bias in sub-aspect: position, importance, diversity
domain: news, academic papers, meeting minutes, movie script, books, posts
different systems: neural-based

dataset
-------
• CNNDM (Nallapati et al., 2016): contains 300K number of online news articles. It has multiple sentences (4.0 on average) as a summary.
• Newsroom (Grusky et al., 2018): contains 1.3M news articles and written summaries by authors and editors from 1998 to 2017. It has both ex- tractive and abstractive summaries.
• XSum (Narayan et al., 2018a): has news articles and their single but abstractive sentence sum- maries mostly written by the original author.
• PeerRead (Kang et al., 2018): consists of sci- entific paper drafts in top-tier computer science venues as well as arxiv.org. We use full text of introduction section as source document and of abstract section as target summaries.
• PubMed (Kedzie et al., 2018): is 25,000 medical journal papers from the PubMed Open Access Subset.7 Unlike PeerRead, full paper except for abstract is used as source documents.
• MScript (Gorinski and Lapata, 2015): is a col- lection of movie scripts from ScriptBase corpus and their corresponding user summaries of the movies.
• BookSum (Mihalcea and Ceylan, 2007): is a dataset of classic books paired to summaries from Grade Saver8 and Cliffs Notes9. Due to a large number of sentences, we only choose the first 1K sentences for source document and the first 50 sentences for target summaries.
• Reddit (Ouyang et al., 2017): is a collection of personal posts from reddit.com. We use a single abstractive summary per post. The same data split from Kedzie et al. (2018) is used.
• AMI (Carletta et al., 2005): is documented meet- ing minutes from a hundred hours of recordings and their abstractive summaries.

====================================================================================
Searching for Effective Neural Extractive Summarization: What Works and What's Next
====================================================================================
1. architectures
1) encoders: CNN, LSTM, Transformer
2) decoders: auto-regressive, non auto-regressive

2. 
1) external transferable knowledge
Glove, BERT, different learning schemas (supervised learning, reinforcement learning)
CNN/DailyMail
position information
2) unsupervised transferable knowledge
(How to use transformer in extractive summerazition?)

3.
1) sentence encoder --> 
doesn't matter

document encoder --> 
LSTM-based
transformer

decoder 
Sequence Labeling(SeqLab): auto-regressive
Pointer Network

======================================================
A Survey on Neural Network-Based Summarization Methods
======================================================
Evaluation
1) intrinsic: system-generated summaries v.s. human-created "gold" summaries: ROUGE, Pyramid
2) extrinsic: based on the performance of the down-stream task

* Neural-Based Summarization Techniques
word embedding: Word2Vec, CW vectors, GloVe
--> encoder: sentence/document: CNNs，RNNs; Recursive Auto-encoder(RAE) 
--> model: selection/generation

1) sentence selection
* diversity of the sentence
* coverage of the input text
 
=========================================================================
Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting
=========================================================================
released code, best pretrained models, output summaries: https://github.com/ChenRocks/fast_abs_rl
a hybrid extractive-abstractive architecture
#   extractor agent  #: 1) select salient sentences
# abstractor network #: 2) rewrite them abstractively

parallel decoding: sentence level & word level
embedded word vectors --(convolutional encoder)--> sentence encoder
--(RNN encoder)--> context-aware representation --(RNN decoder)--> select sentence

# Extractor Agent
## Hierarchical Sentence Representation:
## Sentence Selection: LSTM-RNN to train a Pointer Network
# Abstractor Network

{summarization technique used in dialogue}
knowledge --> abstractive summarization according to key word --> generate sentence with purpose

=================================================================================================
What is this Article about? Extreme Summarization with Topic-aware Convolutional Neural Networks
=================================================================================================
dataset, code, and demo: https://github.com/shashiongithub/XSum
1) draws on information interspersed in various parts of the document
2) multiple levels of abstraction including paraphrasing, fusion, synthesis, and inference

===============================================================================
NEWSROOM: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies
===============================================================================
dataset: https://summari.es

1) retrieves a prototype response from a pre-defined index
2) edits the prototype response according to the differences between the prototype context and the current context

=========================================================================================
Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs
=========================================================================================
entity relation extraction: edge representations formed as paths between nodes

=======================================================================================================
Countering the Effects of Lead Bias in News Summarization via Multi-Stage Training and Auxiliary Losses
=======================================================================================================
make systems sensitive to the importance of content in different parts of the article

1) employs "unbiased" data:
shuffle sentence
2) auxiliary ROUGE-based loss







